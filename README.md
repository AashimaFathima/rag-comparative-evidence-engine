# ğŸ“š RAG Comparative Evidence Engine

A **Retrieval-Augmented Generation (RAG)** system for **extracting, validating, and ranking comparative evidence from research papers** in response to analytical research questions.

This project focuses on **evidence-grounded reasoning**, avoiding hallucinations by grounding all outputs in retrieved academic text.

---

## ğŸš€ What This Project Does

Given a **comparative research question** (for example, *â€œDoes online learning improve academic performance compared to face-to-face learning?â€*), the system:

1. Parses the question into a structured comparison  
2. Retrieves evidence-rich chunks from uploaded research papers  
3. Extracts explicit claims from each paper  
4. Filters out non-result or descriptive claims  
5. Validates claims against the provided evidence  
6. Summarizes and ranks claims by relevance  
7. Returns paper-wise ranked comparative evidence  

All outputs remain **traceable to source text**.

---

## ğŸ§  Motivation

Large language models often:
- Hallucinate facts  
- Mix evidence across sources  
- Produce ungrounded summaries  

This system separates **retrieval, extraction, validation, and ranking** to ensure that responses remain evidence-based and inspectable.

---

## ğŸ—ï¸ System Architecture

```text

User Question
â†“
Query Parser (LLM)
â†“
Evidence-Biased Retrieval (Sentence Transformers)
â†“
Claim Extraction (LLM)
â†“
Result-Only Filtering (Rule-based)
â†“
Claim Validation (LLM)
â†“
Claim Summarization (LLM)
â†“
Claim Ranking (LLM)

```

## ğŸ“ Project Structure


```text

RAG_PROJECT/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ pipeline/
â”‚ â”‚ â”œâ”€â”€ query_parser.py
â”‚ â”‚ â”œâ”€â”€ retrieval.py
â”‚ â”‚ â”œâ”€â”€ claim_extraction.py
â”‚ â”‚ â”œâ”€â”€ claim_validation.py
â”‚ â”‚ â”œâ”€â”€ claim_summarizer.py
â”‚ â”‚ â””â”€â”€ claim_ranker.py
â”‚ â”‚
â”‚ â”œâ”€â”€ prompts/
â”‚ â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ server.py
â”‚ â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ public/
â”‚ â””â”€â”€ package.json
â”‚
â””â”€â”€ README.md

```
---

## ğŸ§ª Evaluation (Overview)

The system has been evaluated using manually curated comparative questions and expected claim sets to verify:

- Claim relevance  
- Evidence faithfulness  
- Comparative correctness  

Evaluation artifacts are intentionally not committed to keep the repository lightweight.  
Metrics and methodology are described conceptually rather than inflated with synthetic numbers.

---

## ğŸ”’ Environment Variables

The backend requires the following environment variable:

```env
GROQ_API_KEY=your_api_key_here

```

## â–¶ï¸ Running Locally

Backend 
```text
cd backend 
pip install -r requirements.txt 
python server.py
```

Frontend 
```text
cd frontend 
npm install 
npm run dev
```

## ğŸ“œ License

This project is for educational and research purposes.

